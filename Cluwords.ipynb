{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import NMF\n",
    "from pyjarowinkler import distance\n",
    "from metrics import *\n",
    "\n",
    "DATA_FILE = 'data/all_data_clean.csv'\n",
    "EMBEDDING_FILE = 'embedding/all_data_clean.bin'\n",
    "RESULTS = 'results'\n",
    "SELECT_DATA = 2  ### 0 titles, 1 titles without stopwords, 2 titles without stopwords and lemmatization\n",
    "SELECT_YEAR = 2015\n",
    "EMBEDDING_BIN = True\n",
    "K_NEIGHBORS = 500\n",
    "N_THREADS = 1\n",
    "THRESHOLD = 0.4\n",
    "COSSINE_FILTER=0.9\n",
    "N_COMPONENTS = 20\n",
    "EMBEDDING_SIZE = 100\n",
    "\n",
    "DATASET_RESULT = 'dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "switcher = {\n",
    "        0: \"pp_title\",\n",
    "        1: \"pp_title_rm_sw\",\n",
    "        2: \"pp_tile_rm_sw_lem\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try:\n",
    "    #os.mkdir('{}'.format(DATASET_RESULT))\n",
    "    #os.mkdir('{}/{}/results'.format(MAIN_PATH, EMBEDDING_RESULTS))\n",
    "    #os.mkdir('{}/{}/datasets'.format(MAIN_PATH, EMBEDDING_RESULTS))\n",
    "    #os.mkdir('{}/{}/datasets/gn_w2v_models'.format(MAIN_PATH, EMBEDDING_RESULTS))\n",
    "#except FileExistsError:\n",
    "#    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>name_citation</th>\n",
       "      <th>doctorate</th>\n",
       "      <th>article</th>\n",
       "      <th>article_year</th>\n",
       "      <th>pp_title</th>\n",
       "      <th>pp_title_rm_sw</th>\n",
       "      <th>pp_tile_rm_sw_lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3172487852109469</td>\n",
       "      <td>Abdelhakim Senhaji Hafid</td>\n",
       "      <td>HAFID, A. S.;A. HAFID;HAFID, ABDELHAKIM;HAFID,...</td>\n",
       "      <td>Computer Science and Operational Research</td>\n",
       "      <td>Performance Management of IEEE 802.15.4 Wirele...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>performance management of ieee wireless sensor...</td>\n",
       "      <td>performance management ieee wireless sensor ne...</td>\n",
       "      <td>performance management ieee wireless sensor ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3172487852109469</td>\n",
       "      <td>Abdelhakim Senhaji Hafid</td>\n",
       "      <td>HAFID, A. S.;A. HAFID;HAFID, ABDELHAKIM;HAFID,...</td>\n",
       "      <td>Computer Science and Operational Research</td>\n",
       "      <td>An Integrated Predictive Mobile-Oriented Bandw...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>an integrated predictive mobileoriented framew...</td>\n",
       "      <td>integrated predictive mobileoriented framework...</td>\n",
       "      <td>integrated predictive mobileoriented framework...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3172487852109469</td>\n",
       "      <td>Abdelhakim Senhaji Hafid</td>\n",
       "      <td>HAFID, A. S.;A. HAFID;HAFID, ABDELHAKIM;HAFID,...</td>\n",
       "      <td>Computer Science and Operational Research</td>\n",
       "      <td>Cross-layer aware joint design of sensing and ...</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>crosslayer aware joint design of sensing and f...</td>\n",
       "      <td>crosslayer aware joint design sensing frame du...</td>\n",
       "      <td>crosslayer aware joint design sensing frame du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3172487852109469</td>\n",
       "      <td>Abdelhakim Senhaji Hafid</td>\n",
       "      <td>HAFID, A. S.;A. HAFID;HAFID, ABDELHAKIM;HAFID,...</td>\n",
       "      <td>Computer Science and Operational Research</td>\n",
       "      <td>An Enhanced Reservation Based Medium Access Co...</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>an enhanced reservation based medium access co...</td>\n",
       "      <td>enhanced reservation based medium access contr...</td>\n",
       "      <td>enhanced reservation based medium access contr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3172487852109469</td>\n",
       "      <td>Abdelhakim Senhaji Hafid</td>\n",
       "      <td>HAFID, A. S.;A. HAFID;HAFID, ABDELHAKIM;HAFID,...</td>\n",
       "      <td>Computer Science and Operational Research</td>\n",
       "      <td>Path-Based QoS Provisioning for Optical Burst ...</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>pathbased qos provisioning for optical burst s...</td>\n",
       "      <td>pathbased qos provisioning optical burst switc...</td>\n",
       "      <td>pathbased qos provisioning optical burst switc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                      name  \\\n",
       "0  3172487852109469  Abdelhakim Senhaji Hafid   \n",
       "1  3172487852109469  Abdelhakim Senhaji Hafid   \n",
       "2  3172487852109469  Abdelhakim Senhaji Hafid   \n",
       "3  3172487852109469  Abdelhakim Senhaji Hafid   \n",
       "4  3172487852109469  Abdelhakim Senhaji Hafid   \n",
       "\n",
       "                                       name_citation  \\\n",
       "0  HAFID, A. S.;A. HAFID;HAFID, ABDELHAKIM;HAFID,...   \n",
       "1  HAFID, A. S.;A. HAFID;HAFID, ABDELHAKIM;HAFID,...   \n",
       "2  HAFID, A. S.;A. HAFID;HAFID, ABDELHAKIM;HAFID,...   \n",
       "3  HAFID, A. S.;A. HAFID;HAFID, ABDELHAKIM;HAFID,...   \n",
       "4  HAFID, A. S.;A. HAFID;HAFID, ABDELHAKIM;HAFID,...   \n",
       "\n",
       "                                   doctorate  \\\n",
       "0  Computer Science and Operational Research   \n",
       "1  Computer Science and Operational Research   \n",
       "2  Computer Science and Operational Research   \n",
       "3  Computer Science and Operational Research   \n",
       "4  Computer Science and Operational Research   \n",
       "\n",
       "                                             article  article_year  \\\n",
       "0  Performance Management of IEEE 802.15.4 Wirele...        2015.0   \n",
       "1  An Integrated Predictive Mobile-Oriented Bandw...        2014.0   \n",
       "2  Cross-layer aware joint design of sensing and ...        2016.0   \n",
       "3  An Enhanced Reservation Based Medium Access Co...        2012.0   \n",
       "4  Path-Based QoS Provisioning for Optical Burst ...        2011.0   \n",
       "\n",
       "                                            pp_title  \\\n",
       "0  performance management of ieee wireless sensor...   \n",
       "1  an integrated predictive mobileoriented framew...   \n",
       "2  crosslayer aware joint design of sensing and f...   \n",
       "3  an enhanced reservation based medium access co...   \n",
       "4  pathbased qos provisioning for optical burst s...   \n",
       "\n",
       "                                      pp_title_rm_sw  \\\n",
       "0  performance management ieee wireless sensor ne...   \n",
       "1  integrated predictive mobileoriented framework...   \n",
       "2  crosslayer aware joint design sensing frame du...   \n",
       "3  enhanced reservation based medium access contr...   \n",
       "4  pathbased qos provisioning optical burst switc...   \n",
       "\n",
       "                                   pp_tile_rm_sw_lem  \n",
       "0  performance management ieee wireless sensor ne...  \n",
       "1  integrated predictive mobileoriented framework...  \n",
       "2  crosslayer aware joint design sensing frame du...  \n",
       "3  enhanced reservation based medium access contr...  \n",
       "4  pathbased qos provisioning optical burst switc...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data = pd.read_csv(DATA_FILE, sep='|', index_col=0)\n",
    "csv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['performance management ieee wireless sensor network precision agriculture',\n",
       " 'mogamap2 multiobjective mapping algorithm parameter control optimize area performance power consumption fpga',\n",
       " 'interfpga communication bus error detection dynamic clock phase adjustment',\n",
       " 'bone dentistry digital xray bµaddx software pilot study analysis bone density digital dental xrays',\n",
       " 'model checking cml tool development industrial application',\n",
       " 'hefestos intelligent system applied ubiquitous accessibility',\n",
       " 'logadm approach dynamic log analysis',\n",
       " 'integrated infrastructure ubiquitous learning',\n",
       " 'fault domainbased testing imperfect situation heuristic approach case study',\n",
       " 'generation complete test suite mealy inputoutput transition system']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = [x.get(switcher.get(SELECT_DATA)) for index, x in csv_data.iterrows() if csv_data.at[index, 'article_year'] == SELECT_YEAR]\n",
    "n_documents = len(dataset)\n",
    "csv_data = None\n",
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.9 ms, sys: 0 ns, total: 42.9 ms\n",
      "Wall time: 41.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=EMBEDDING_BIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter embedding space to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['025v',\n",
       " '12conjecture',\n",
       " '130nm',\n",
       " '1a',\n",
       " '1h',\n",
       " '20nm',\n",
       " '24',\n",
       " '2d',\n",
       " '2d3d',\n",
       " '2d4d']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_words = CountVectorizer().fit(dataset).get_feature_names()\n",
    "dataset_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_vector = {}\n",
    "for word in dataset_words:\n",
    "    try:\n",
    "        words_vector[word] = model[word]         \n",
    "    except KeyError:\n",
    "        continue\n",
    "dataset_words = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cluwords 2732\n"
     ]
    }
   ],
   "source": [
    "n_words = len(words_vector)\n",
    "print('Number of cluwords {}'.format(n_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save my word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('mkdir -p ' + DATASET_RESULT)\n",
    "file = open(\"\"\"{}/{}_{}.txt\"\"\".format(DATASET_RESULT, switcher.get(SELECT_DATA),  SELECT_YEAR), 'w')\n",
    "file.write('{} {}\\n'.format(n_words, str(EMBEDDING_SIZE)))\n",
    "for index, word_vec in words_vector.items():\n",
    "    file.write(\"%s\\n\" % str(index + ' ' + \" \".join([str(round(x, 9)) for x in word_vec.tolist()])))\n",
    "file.close()\n",
    "model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Cluwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00521728,  0.02028853, -0.01500222, ..., -0.02085901,\n",
       "         0.0051961 , -0.06698267],\n",
       "       [ 0.01002557,  0.02945558, -0.0240077 , ..., -0.01943226,\n",
       "         0.00024938, -0.04257441],\n",
       "       [ 0.0146225 ,  0.04320482, -0.02451873, ..., -0.02097077,\n",
       "         0.00505415, -0.05099378],\n",
       "       ...,\n",
       "       [ 0.01427482,  0.03594904, -0.03064483, ..., -0.02900777,\n",
       "         0.01136413, -0.06841376],\n",
       "       [ 0.01087763,  0.03139909, -0.02050509, ..., -0.02117275,\n",
       "         0.00259017, -0.05183641],\n",
       "       [ 0.01641259,  0.07519466, -0.04875149, ..., -0.04672395,\n",
       "         0.01962009, -0.12792523]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space_vector = [np.array([round(y,9) for y in words_vector[x].tolist()]) for x in words_vector]\n",
    "space_vector = np.array(space_vector)\n",
    "vocab_cluwords = np.asarray([x for x in words_vector])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 343 ms, sys: 180 ms, total: 523 ms\n",
      "Wall time: 226 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nbrs = NearestNeighbors(n_neighbors=K_NEIGHBORS, algorithm='auto', metric='cosine', n_jobs=N_THREADS).fit(space_vector)\n",
    "distances, indices = nbrs.kneighbors(space_vector)\n",
    "space_vector = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Cluwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cluwords = np.zeros((n_words, n_words), dtype=np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if THRESHOLD:\n",
    "    for p in range(0, n_words):\n",
    "        for i, k in enumerate(indices[p]):\n",
    "            # .875, .75, .625, .50\n",
    "            if 1 - distances[p][i] >= THRESHOLD:\n",
    "                list_cluwords[p][k] = round(1 - distances[p][i], 2)\n",
    "            else:\n",
    "                list_cluwords[p][k] = 0.0\n",
    "else:\n",
    "    for p in range(0, n_words):\n",
    "        for i, k in enumerate(indices[p]):\n",
    "            list_cluwords[p][k] = round(1 - distances[p][i], 2)\n",
    "distances, indices = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savez_compressed('cluwords.npz', data=list_cluwords, index=np.asarray(vocab_cluwords), cluwords=np.asarray(vocab_cluwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf shape (2609, 2732)\n"
     ]
    }
   ],
   "source": [
    "tf_vectorizer = CountVectorizer(max_features=n_words, binary=False, vocabulary=vocab_cluwords)\n",
    "tf = tf_vectorizer.fit_transform(dataset)\n",
    "n_cluwords = len(vocab_cluwords)\n",
    "print('tf shape {}'.format(tf.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 235 ms, sys: 12 ms, total: 247 ms\n",
      "Wall time: 246 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hyp_aux = []\n",
    "for w in range(0, n_cluwords):\n",
    "    hyp_aux.append(np.asarray(list_cluwords[w], dtype=np.float16))\n",
    "\n",
    "hyp_aux = np.asarray(hyp_aux, dtype=np.float32)\n",
    "hyp_aux = csr_matrix(hyp_aux, shape=hyp_aux.shape, dtype=np.float32)  # ?test sparse matrix!\n",
    "\n",
    "cluwords_tf_idf = np.dot(tf, np.transpose(hyp_aux))\n",
    "cluwords_tf_idf = tf.dot(hyp_aux.transpose())\n",
    "tf = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hyp_aux = hyp_aux.todense()\\n\\nprint(\\'Dot tf and hyp_aux\\')\\n_dot = np.dot(tf, np.transpose(hyp_aux))  # np.array n_documents x n_cluwords # Correct!\\nend = timeit.default_timer()\\n\\nprint(\\'Divide hyp_aux by itself\\')\\nwith warnings.catch_warnings():\\n    warnings.simplefilter(\"ignore\")\\n    # pdb.set_trace()\\n    # self.hyp_aux = self.hyp_aux.todense()\\n    # pdb.set_trace()\\n    bin_hyp_aux = np.nan_to_num(np.divide(hyp_aux, hyp_aux))\\n    \\nprint(\\'Dot tf and bin hyp_aux\\')\\n_dot_bin = np.dot(tf, np.transpose(bin_hyp_aux))\\n\\nprint(\\'Divide _dot and _dot_bin\\')\\nwith warnings.catch_warnings():\\n    warnings.simplefilter(\"ignore\")\\n    mu_hyp = np.nan_to_num(np.divide(_dot, _dot_bin))\\n\\nstart = timeit.default_timer()\\nprint(\\'Sum\\')\\ncluwords_idf = np.sum(mu_hyp, axis=0)\\n\\nstart = timeit.default_timer()\\nprint(\\'log\\')\\ncluwords_idf = np.log10(np.divide(n_documents, cluwords_idf))'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''hyp_aux = hyp_aux.todense()\n",
    "\n",
    "print('Dot tf and hyp_aux')\n",
    "_dot = np.dot(tf, np.transpose(hyp_aux))  # np.array n_documents x n_cluwords # Correct!\n",
    "end = timeit.default_timer()\n",
    "\n",
    "print('Divide hyp_aux by itself')\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    # pdb.set_trace()\n",
    "    # self.hyp_aux = self.hyp_aux.todense()\n",
    "    # pdb.set_trace()\n",
    "    bin_hyp_aux = np.nan_to_num(np.divide(hyp_aux, hyp_aux))\n",
    "    \n",
    "print('Dot tf and bin hyp_aux')\n",
    "_dot_bin = np.dot(tf, np.transpose(bin_hyp_aux))\n",
    "\n",
    "print('Divide _dot and _dot_bin')\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    mu_hyp = np.nan_to_num(np.divide(_dot, _dot_bin))\n",
    "\n",
    "start = timeit.default_timer()\n",
    "print('Sum')\n",
    "cluwords_idf = np.sum(mu_hyp, axis=0)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "print('log')\n",
    "cluwords_idf = np.log10(np.divide(n_documents, cluwords_idf))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=2609 and n_features=2732...\n",
      "CPU times: user 1min 54s, sys: 1.83 s, total: 1min 55s\n",
      "Wall time: 1min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"\\nFitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_documents, n_cluwords))\n",
    "nmf = NMF(n_components=N_COMPONENTS, random_state=1, alpha=.1, l1_ratio=.5, max_iter=1500).fit(cluwords_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = []\n",
    "n_top_words = 101\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    top = ''\n",
    "    top2 = ''\n",
    "    top += ' '.join([vocab_cluwords[i]\n",
    "                     for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    top2 += ''.join(str(sorted(topic)[:-n_top_words - 1:-1]))\n",
    "\n",
    "    topics.append(str(top))\n",
    "cluwords_freq, cluwords_docs, n_docs = count_tf_idf_repr(topics, vocab_cluwords, cluwords_tf_idf.transpose())\n",
    "nmf = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_t = []\n",
    "for topic in topics:\n",
    "    topic_t = topic.split(' ')\n",
    "    topics_t.append(topic_t)\n",
    "topics = topics_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_topics = []\n",
    "top = 101\n",
    "for topic in topics:\n",
    "    topic_top = topic[:top]\n",
    "    one_hot_topic = np.zeros(len(vocab_cluwords))\n",
    "    for word in topic_top:\n",
    "        index_vocab = np.argwhere(vocab_cluwords == word)[0]\n",
    "        one_hot_topic[index_vocab] = 1\n",
    "\n",
    "    one_hot_topics.append(one_hot_topic)\n",
    "\n",
    "one_hot_topics = np.array(one_hot_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_t = []\n",
    "for topic in topics:\n",
    "    filtered_topic = []\n",
    "    insert_word = np.ones(len(topic))\n",
    "    for w_i in range(0, len(topic)-1):\n",
    "        if insert_word[w_i]:\n",
    "            filtered_topic.append(topic[w_i])\n",
    "            for w_j in range((w_i + 1), len(topic)):\n",
    "                if distance.get_jaro_distance(topic[w_i], topic[w_j], winkler=True, scaling=0.1) > 0.75:\n",
    "                    insert_word[w_j] = 0\n",
    "\n",
    "    topics_t.append(filtered_topic)\n",
    "topics = topics_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('mkdir -p ' + RESULTS)\n",
    "for t in [5, 10, 20]:\n",
    "    with open('{}/result_topic_{}.txt'.format(RESULTS, t), 'w') as f_res:\n",
    "        f_res.write('Topics {}\\n'.format(t))\n",
    "        f_res.write('Topics:\\n')\n",
    "        topics_t = []\n",
    "        for topic in topics:\n",
    "            topics_t.append(topic[:t])\n",
    "            for word in topic[:t]:\n",
    "                f_res.write('{} '.format(word))\n",
    "\n",
    "            f_res.write('\\n')\n",
    "        coherence = get_coherence(topics_t, cluwords_freq, cluwords_docs)\n",
    "        f_res.write('Coherence: {} ({})\\n'.format(np.round(np.mean(coherence), 4), np.round(np.std(coherence), 4)))\n",
    "        f_res.write('{}\\n'.format(coherence))\n",
    "\n",
    "        pmi, npmi = get_pmi(topics=topics_t, word_frequency=cluwords_freq, term_docs=cluwords_docs, n_docs=n_docs, n_top_words=t)\n",
    "        f_res.write('PMI: {} ({})\\n'.format(np.round(np.mean(pmi), 4), np.round(np.std(pmi), 4)))\n",
    "        f_res.write('{}\\n'.format(pmi))\n",
    "        f_res.write('NPMI:\\n')\n",
    "        for score in npmi:\n",
    "            f_res.write('{}\\n'.format(score))\n",
    "\n",
    "        f_res.write('avg NPMI: {} ({})\\n'.format(np.round(np.mean(npmi), 4), np.round(np.std(npmi), 4)))\n",
    "\n",
    "        w2v_l1 = get_w2v_metric(topics, t, DATASET_RESULT, 'l1_dist', \"{}_{}\".format(switcher.get(SELECT_DATA),  SELECT_YEAR))\n",
    "        f_res.write('W2V-L1: {} ({})\\n'.format(np.round(np.mean(w2v_l1), 4), np.round(np.std(w2v_l1), 4)))\n",
    "        f_res.write('{}\\n'.format(w2v_l1))\n",
    "\n",
    "        f_res.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
