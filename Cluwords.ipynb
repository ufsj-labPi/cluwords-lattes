{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import codecs\n",
    "from numbers import Number\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "import timeit\n",
    "import scipy as sp\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy.spatial.distance as sci_dist\n",
    "import timeit\n",
    "import warnings\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from pyjarowinkler import distance\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from time import time\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "MAIN_PATH='.'\n",
    "EMBEDDING_RESULTS = 'google'\n",
    "PATH_TO_SAVE_RESULTS = '{}/{}/results'.format(MAIN_PATH, EMBEDDING_RESULTS)\n",
    "PATH_TO_SAVE_MODEL = '{}/{}/datasets/gn_w2v_models'.format(MAIN_PATH, EMBEDDING_RESULTS)\n",
    "EMBEDDINGS_FILE_PATH = '{}/GoogleNews-vectors-negative300.bin'.format(MAIN_PATH)\n",
    "EMBEDDINGS_BIN_TYPE = True\n",
    "DATASET = 'all_titles'\n",
    "DATASET_PATH = '../data/all_titles.txt'\n",
    "N_THREADS = 15\n",
    "N_TOPICS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir('{}/{}'.format(MAIN_PATH, EMBEDDING_RESULTS))\n",
    "    os.mkdir('{}/{}/results'.format(MAIN_PATH, EMBEDDING_RESULTS))\n",
    "    os.mkdir('{}/{}/datasets'.format(MAIN_PATH, EMBEDDING_RESULTS))\n",
    "    os.mkdir('{}/{}/datasets/gn_w2v_models'.format(MAIN_PATH, EMBEDDING_RESULTS))\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the w2v model for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset\n",
    "def read_raw_dataset(dataset):\n",
    "    arq = open(dataset, 'r')\n",
    "    doc = arq.readlines()\n",
    "    arq.close()\n",
    "    documents = list(map(str.rstrip, doc))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read embedding\n",
    "def read_embedding(embedding_file_path, binary):\n",
    "    t0 = time()\n",
    "    model = KeyedVectors.load_word2vec_format(embedding_file_path, binary=binary)\n",
    "    print('Embedding model read in %0.3fs.' % (time() - t0))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_models(dataset_path = DATASET_PATH, dataset=DATASET,\n",
    "                            embedding_file_path=EMBEDDINGS_FILE_PATH,\n",
    "                            embedding_type=EMBEDDINGS_BIN_TYPE,\n",
    "                            path_to_save_model=PATH_TO_SAVE_MODEL):\n",
    "    documents = read_raw_dataset(dataset_path)\n",
    "    # Count the words in dataset\n",
    "    dataset_cv = CountVectorizer().fit(documents)\n",
    "    dataset_words = dataset_cv.get_feature_names()\n",
    "    \n",
    "    # Select just the words in dataset from Google News Word2Vec Model\n",
    "    words_values = []\n",
    "    model = read_embedding(embedding_file_path, embedding_type)\n",
    "    for i in dataset_words:\n",
    "        aux = [i + ' ']\n",
    "        try:\n",
    "            for k in model[i]:\n",
    "                aux[0] += str(k) + ' '\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        words_values.append(aux[0])\n",
    "\n",
    "    n_words = len(words_values)  # Number of words selected\n",
    "\n",
    "    print('{}:{}'.format(dataset, n_words))\n",
    "\n",
    "    # save .txt model\n",
    "    os.system('mkdir -p ' + path_to_save_model)\n",
    "    file = open(\"\"\"{}/{}.txt\"\"\".format(path_to_save_model, dataset), 'w+')\n",
    "    file.write('{0} {1}\\n'.format(n_words, '300'))\n",
    "    for word_vec in words_values:\n",
    "        file.write(\"%s\\n\" % word_vec)\n",
    "\n",
    "    return n_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter embedding space to ../data/all_titles.txt dataset...\n"
     ]
    }
   ],
   "source": [
    "print('Filter embedding space to {} dataset...'.format(DATASET_PATH))\n",
    "n_words = create_embedding_models(dataset=DATASET,\n",
    "                                  embedding_file_path=EMBEDDINGS_FILE_PATH,\n",
    "                                  embedding_type=EMBEDDINGS_BIN_TYPE,\n",
    "                                  path_to_save_model=PATH_TO_SAVE_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alfa Knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cluwords(labels_array, n_words, k_neighbors, distances, indices, threshold):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        -----------\n",
    "        Save the cluwords of each word to csv using pandas. Dataframe.\n",
    "        \n",
    "        \"\"\"\n",
    "        list_cluwords = np.zeros((n_words, n_words), dtype=np.float16)\n",
    "\n",
    "        # Check if cosine limit was set\n",
    "        if threshold:\n",
    "            for p in range(0, n_words):\n",
    "                for i, k in enumerate(indices[p]):\n",
    "                    # .875, .75, .625, .50\n",
    "                    if 1 - distances[p][i] >= threshold:\n",
    "                        list_cluwords[p][k] = round(1 - distances[p][i], 2)\n",
    "                    else:\n",
    "                        list_cluwords[p][k] = 0.0\n",
    "        else:\n",
    "            for p in range(0, n_words):\n",
    "                for i, k in enumerate(indices[p]):\n",
    "                    list_cluwords[p][k] = round(1 - distances[p][i], 2)\n",
    "\n",
    "        np.savez_compressed('cluwords.npz',\n",
    "                            data=list_cluwords,\n",
    "                            index=np.asarray(labels_array),\n",
    "                            cluwords=np.asarray(labels_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_vector_matrix(vector_file, n_words):\n",
    "    \"\"\"Read a GloVe array from sys.argv[1] and return its vectors and labels as arrays\"\"\"\n",
    "    numpy_arrays = []\n",
    "    labels_array = []\n",
    "\n",
    "    with codecs.open(vector_file, 'r', 'utf-8') as f:\n",
    "        _ = next(f)  # Skip the first line\n",
    "\n",
    "        for c, r in enumerate(f):\n",
    "            sr = r.split()\n",
    "            labels_array.append(sr[0])\n",
    "            numpy_arrays.append(np.array([float(i) for i in sr[1:]]))\n",
    "\n",
    "            if c == n_words:\n",
    "                return np.array(numpy_arrays), labels_array\n",
    "\n",
    "    return np.array(numpy_arrays), labels_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cosine_cluwords(input_vector_file, n_words, k_neighbors, threshold, n_jobs):\n",
    "        input_vector_file = input_vector_file\n",
    "        df, labels_array = build_word_vector_matrix(input_vector_file, n_words)\n",
    "        print('NearestNeighbors K={}'.format(k_neighbors))\n",
    "        start = timeit.default_timer()\n",
    "        nbrs = NearestNeighbors(n_neighbors=k_neighbors, algorithm='auto', metric='cosine', n_jobs=n_jobs).fit(\n",
    "            df)\n",
    "        end = timeit.default_timer()\n",
    "        print('Time {}'.format(end - start))\n",
    "        print('NN Distaces')\n",
    "        start = timeit.default_timer()\n",
    "        distances, indices = nbrs.kneighbors(df)\n",
    "        end = timeit.default_timer()\n",
    "        print('Time {}'.format(end - start))\n",
    "        print('Saving cluwords')\n",
    "\n",
    "        save_cluwords(labels_array, n_words, k_neighbors, distances, indices, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Cluwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(dataset_file_path):\n",
    "        arq = open(dataset_file_path, 'r')\n",
    "        doc = arq.readlines()\n",
    "        arq.close()\n",
    "\n",
    "        documents = list(map(str.rstrip, doc))\n",
    "        n_documents = len(documents)\n",
    "        return documents, n_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluwords_dataframe(embedding_file_path, n_words, k_neighbors, threshold=.85, n_jobs=1, verbose=0):\n",
    "    print('kNN...')\n",
    "    create_cosine_cluwords(input_vector_file=embedding_file_path,\n",
    "                               n_words=n_words,\n",
    "                               k_neighbors=k_neighbors, threshold=threshold, n_jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluwords(dataset_file_path, n_words, path_to_save_cluwords,\n",
    "        cossine_filter=1.0):\n",
    "    path_to_save_cluwords_tfidf = path_to_save_cluwords + '/cluwords_features.libsvm'\n",
    "    n_words = n_words\n",
    "    cossine_filter = cossine_filter\n",
    "    loaded = np.load('cluwords.npz')\n",
    "    vocab = loaded['index']\n",
    "    vocab_cluwords = loaded['cluwords']\n",
    "    cluwords_data = loaded['data']\n",
    "\n",
    "    print('Matrix{}'.format(cluwords_data.shape))\n",
    "    del loaded\n",
    "    print('\\nCosine Filter: {}'.format(cossine_filter))\n",
    "\n",
    "    documents, n_documents = read_input(dataset_file_path)\n",
    "    return documents, n_documents, vocab, vocab_cluwords, cluwords_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_tf(documents, n_words, vocab, binary=False, dtype=np.float32):\n",
    "        tf_vectorizer = CountVectorizer(max_features=n_words, binary=binary, vocabulary=vocab)\n",
    "        tf = tf_vectorizer.fit_transform(documents)\n",
    "        return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluwords_tf(documents, n_words, vocab, vocab_cluwords, cluwords_data, binary):\n",
    "        start = timeit.default_timer()\n",
    "        tf = raw_tf(documents, n_words, vocab, binary)\n",
    "\n",
    "        print('tf shape {}'.format(tf.shape))\n",
    "\n",
    "        hyp_aux = []\n",
    "        for w in range(0, len(vocab_cluwords)):\n",
    "            hyp_aux.append(np.asarray(cluwords_data[w], dtype=np.float16))\n",
    "\n",
    "        hyp_aux = np.asarray(hyp_aux, dtype=np.float32)\n",
    "        hyp_aux = csr_matrix(hyp_aux, shape=hyp_aux.shape, dtype=np.float32)  # test sparse matrix!\n",
    "\n",
    "        cluwords_tf_idf = np.dot(tf, np.transpose(hyp_aux))\n",
    "        cluwords_tf_idf = tf.dot(hyp_aux.transpose())\n",
    "\n",
    "        end = timeit.default_timer()\n",
    "        print(\"Cluwords TF done in %0.3fs.\" % (end - start))\n",
    "        return cluwords_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluwords_idf(documents, n_documents, n_words, vocab, vocab_cluwords, cluwords_data):\n",
    "        start = timeit.default_timer()\n",
    "        print('Read data')\n",
    "        tf = raw_tf(binary, dtype=np.float32)\n",
    "        hyp_aux = hyp_aux.todense()\n",
    "\n",
    "        end = timeit.default_timer()\n",
    "        print('Time {}'.format(end - start))\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "        print('Dot tf and hyp_aux')\n",
    "        _dot = np.dot(tf, np.transpose(hyp_aux))  # np.array n_documents x n_cluwords # Correct!\n",
    "        end = timeit.default_timer()\n",
    "        print('Time {}'.format(end - start))\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "        print('Divide hyp_aux by itself')\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            # pdb.set_trace()\n",
    "            # self.hyp_aux = self.hyp_aux.todense()\n",
    "            # pdb.set_trace()\n",
    "            bin_hyp_aux = np.nan_to_num(np.divide(hyp_aux, hyp_aux))\n",
    "        end = timeit.default_timer()\n",
    "        print('Time {}'.format(end - start))\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "        print('Dot tf and bin hyp_aux')\n",
    "        _dot_bin = np.dot(tf, np.transpose(bin_hyp_aux))\n",
    "\n",
    "        end = timeit.default_timer()\n",
    "        print('Time {}'.format(end - start))\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "        print('Divide _dot and _dot_bin')\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            mu_hyp = np.nan_to_num(np.divide(_dot, _dot_bin))\n",
    "        end = timeit.default_timer()\n",
    "        print('Time {}'.format(end - start))\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "        print('Sum')\n",
    "        cluwords_idf = np.sum(mu_hyp, axis=0)\n",
    "        end = timeit.default_timer()\n",
    "        print('Time {}'.format(end - start))\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "        print('log')\n",
    "        cluwords_idf = np.log10(np.divide(n_documents, cluwords_idf))\n",
    "        end = timeit.default_timer()\n",
    "        print('Time {}'.format(end - start))\n",
    "        return cluwords_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluwords_fit_transform(documents, n_documents, n_words, vocab, vocab_cluwords, cluwords_data, binary):\n",
    "        \"\"\"Compute cluwords tfidf.\"\"\"\n",
    "\n",
    "        # Set number of cluwords\n",
    "        n_cluwords = n_words\n",
    "\n",
    "       \n",
    "        # Set vocabulary of cluwords\n",
    "        n_cluwords = len(vocab_cluwords)\n",
    "        print('Number of cluwords {}'.format(len(vocab_cluwords)))\n",
    "        print('Matrix{}'.format(cluwords_data.shape))\n",
    "\n",
    "        print('\\nComputing TF...')\n",
    "        cluwords_tf_idf = cluwords_tf(documents, n_words, vocab, vocab_cluwords, cluwords_data, binary)\n",
    "        #print('\\nComputing IDF...')\n",
    "        #cluwords_tf_idf = cluwords_idf(documents, n_documents, n_words, vocab, vocab_cluwords, cluwords_data)\n",
    "        \n",
    "        print(cluwords_tf_idf.shape)\n",
    "        return cluwords_tf_idf, n_cluwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words(model, feature_names, n_top_words):\n",
    "    topico = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top = ''\n",
    "        top2 = ''\n",
    "        top += ' '.join([feature_names[i]\n",
    "                         for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        top2 += ''.join(str(sorted(topic)[:-n_top_words - 1:-1]))\n",
    "\n",
    "        topico.append(str(top))\n",
    "\n",
    "    return topico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_topics(topics):\n",
    "    topics_t = []\n",
    "    for topic in topics:\n",
    "        topic_t = topic.split(' ')\n",
    "        topics_t.append(topic_t)\n",
    "\n",
    "    return topics_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbors(X_topic, X_raw, vocab, n_topics, dataset):\n",
    "    X = _raw_tf(X_raw, vocab, binary=True)\n",
    "    neigh = NearestNeighbors(n_neighbors=n_topics, algorithm='auto', metric='cosine')\n",
    "    neigh.fit(X_topic)\n",
    "    dist, ind = neigh.kneighbors(X)\n",
    "    output = open('document_distribution_{}'.format(dataset), 'w')\n",
    "    for doc in range(0, dist.shape[0]):\n",
    "        topic_dist = np.zeros(dist.shape[1])\n",
    "        for index in range(0, dist.shape[1]):\n",
    "            topic_index = ind[index]\n",
    "            topic_dist[topic_index] = 1. - dist[doc, topic_index]\n",
    "\n",
    "        total_dist = np.sum(topic_dist)\n",
    "        output.write('{} '.format(doc))\n",
    "        for index in range(0, dist.shape[1]):\n",
    "            output.write(' {}:{}'.format(index, round(topic_dist[index]/total_dist if total_dist > 0 else .0, 4)))\n",
    "\n",
    "        output.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_redundant_words(topics):\n",
    "    topics_t = []\n",
    "    for topic in topics:\n",
    "        filtered_topic = []\n",
    "        insert_word = np.ones(len(topic))\n",
    "        for w_i in range(0, len(topic)-1):\n",
    "            if insert_word[w_i]:\n",
    "                filtered_topic.append(topic[w_i])\n",
    "                for w_j in range((w_i + 1), len(topic)):\n",
    "                    if distance.get_jaro_distance(topic[w_i], topic[w_j], winkler=True, scaling=0.1) > 0.75:\n",
    "                        insert_word[w_j] = 0\n",
    "\n",
    "        topics_t.append(filtered_topic)\n",
    "\n",
    "    return topics_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coherence(topic, word_frequency, term_docs):\n",
    "        coherence = []\n",
    "\n",
    "        for t in range(len(topic)):\n",
    "            topico = topic[t]\n",
    "            top_w = topico.split(\" \")\n",
    "\n",
    "            coherence_t = 0.0\n",
    "            for i in range(1, len(top_w)):\n",
    "                for j in range(0, i):\n",
    "                    cont_wi = word_frequency[top_w[j]]\n",
    "                    cont_wi_wj = float(\n",
    "                        len(term_docs[top_w[j]].intersection(term_docs[top_w[i]])))\n",
    "                    coherence_t += np.log((cont_wi_wj + 1.0) / cont_wi)\n",
    "\n",
    "            coherence.append(coherence_t)\n",
    "\n",
    "        return coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi(topics, word_frequency, term_docs, n_docs, n_top_words):\n",
    "        pmi = []\n",
    "        npmi = []\n",
    "\n",
    "        n_top_words = float(n_top_words)\n",
    "\n",
    "        for t in range(len(topics)):\n",
    "            top_w = topics[t]\n",
    "            # top_w = topico.split(' ')\n",
    "\n",
    "            pmi_t = 0.0\n",
    "            npmi_t = 0.0\n",
    "\n",
    "            for j in range(1, len(top_w)):\n",
    "                for i in range(0, j):\n",
    "                    ti = top_w[i]\n",
    "                    tj = top_w[j]\n",
    "\n",
    "                    c_i = word_frequency[ti]\n",
    "                    c_j = word_frequency[tj]\n",
    "                    c_i_and_j = len(term_docs[ti].intersection(term_docs[tj]))\n",
    "\n",
    "                    pmi_t += np.log(((c_i_and_j + 1.0) / float(n_docs)) /\n",
    "                                    ((c_i * c_j) / float(n_docs) ** 2))\n",
    "\n",
    "                    npmi_t += -1.0 * np.log((c_i_and_j + 0.01) / float(n_docs))\n",
    "\n",
    "            peso = 1.0 / (n_top_words * (n_top_words - 1.0))\n",
    "\n",
    "            pmi.append(peso * pmi_t)\n",
    "            npmi.append(pmi_t / npmi_t)\n",
    "\n",
    "        return pmi, npmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_metric(topics, t, path_to_save_model, distance_type, dataset, embedding_type=False):\n",
    "        word_vectors = KeyedVectors.load_word2vec_format(fname='{}/{}.txt'.format(path_to_save_model, dataset), binary=embedding_type)\n",
    "        model = word_vectors.wv\n",
    "        values = []\n",
    "\n",
    "        for topic in topics:\n",
    "            words = topic.split(' ')\n",
    "            value = Evaluation._calc_dist_2(words, model, distance_type, t)\n",
    "            values.append(value)\n",
    "\n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tf_idf_repr(topics, cw_words, tf_idf_t):\n",
    "        cw_frequency = {}\n",
    "        cw_docs = {}\n",
    "        for iter_topic in topics:\n",
    "            topic = iter_topic.split(' ')\n",
    "            for word in topic:\n",
    "                word_index = np.where(cw_words == word)[0]\n",
    "                cw_frequency[word] = float(tf_idf_t[word_index].getnnz(1))\n",
    "                cw_docs[word] = set(tf_idf_t[word_index].nonzero()[1])\n",
    "\n",
    "        n_docs = 0\n",
    "        for _cw in range(tf_idf_t.shape[0]):\n",
    "            n_docs += float(tf_idf_t[_cw].getnnz(1))\n",
    "\n",
    "        return cw_frequency, cw_docs, n_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(cluwords_freq, cluwords_docs, path_to_save_results, topics, n_docs):\n",
    "    print(path_to_save_results)\n",
    "    for t in [5, 10, 20]:\n",
    "        with open('{}/result_topic_{}.txt'.format(path_to_save_results, t), 'w') as f_res:\n",
    "            f_res.write('Topics {}\\n'.format(t))\n",
    "            f_res.write('Topics:\\n')\n",
    "            topics_t = []\n",
    "            for topic in topics:\n",
    "                topics_t.append(topic[:t])\n",
    "                for word in topic[:t]:\n",
    "                    f_res.write('{} '.format(word))\n",
    "\n",
    "                f_res.write('\\n')\n",
    "\n",
    "            coherence = coherence(topics_t, cluwords_freq, cluwords_docs)\n",
    "            f_res.write('Coherence: {} ({})\\n'.format(np.round(np.mean(coherence), 4),\n",
    "                                                       np.round(np.std(coherence), 4)))\n",
    "            f_res.write('{}\\n'.format(coherence))\n",
    "\n",
    "            pmi, npmi = pmi(topics=topics_t,\n",
    "                                       word_frequency=cluwords_freq,\n",
    "                                       term_docs=cluwords_docs,\n",
    "                                       n_docs=n_docs,\n",
    "                                       n_top_words=t)\n",
    "            f_res.write('PMI: {} ({})\\n'.format(np.round(np.mean(pmi), 4), np.round(np.std(pmi), 4)))\n",
    "            f_res.write('{}\\n'.format(pmi))\n",
    "            f_res.write('NPMI:\\n')\n",
    "            for score in npmi:\n",
    "                f_res.write('{}\\n'.format(score))\n",
    "\n",
    "            f_res.write('avg NPMI: {} ({})\\n'.format(np.round(np.mean(npmi), 4), np.round(np.std(npmi), 4)))\n",
    "\n",
    "            w2v_l1 = Evaluation.w2v_metric(topics, t, path_to_save_model, 'l1_dist', dataset)\n",
    "            f_res.write('W2V-L1: {} ({})\\n'.format(np.round(np.mean(w2v_l1), 4), np.round(np.std(w2v_l1), 4)))\n",
    "            f_res.write('{}\\n'.format(w2v_l1))\n",
    "\n",
    "            f_res.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def generate_topics(dataset, word_count, path_to_save_model, dataset_path,\n",
    "#                    path_to_save_results, n_threads, k, threshold, cossine_filter,\n",
    "#                    n_components, algorithm_type):\n",
    "    # Path to files and directories\n",
    "dataset=DATASET\n",
    "dataset_file_path=DATASET_PATH\n",
    "word_count=n_words\n",
    "path_to_save_model=PATH_TO_SAVE_MODEL\n",
    "path_to_save_results=PATH_TO_SAVE_RESULTS\n",
    "n_threads=N_THREADS\n",
    "algorithm_type=EMBEDDINGS_BIN_TYPE\n",
    "# k=n_words,\n",
    "k=500\n",
    "threshold=0.4\n",
    "cossine_filter=0.9\n",
    "n_components=N_TOPICS\n",
    "embedding_file_path = \"\"\"{}/{}.txt\"\"\".format(path_to_save_model, dataset)\n",
    "path_to_save_results = '{}/{}'.format(path_to_save_results, dataset)\n",
    "\n",
    "try:\n",
    "    os.mkdir('{}'.format(path_to_save_results))\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "cluwords_dataframe(embedding_file_path, word_count, k, threshold=threshold, n_jobs=n_threads, verbose=0)\n",
    "documents, n_documents, vocab, vocab_cluwords, cluwords_data = create_cluwords(dataset_file_path=dataset_file_path, path_to_save_cluwords=path_to_save_results, n_words=word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Computing TFIDF...')\n",
    "cluwords_tfidf, n_cluwords = cluwords_fit_transform(documents, n_documents, n_words, vocab, vocab_cluwords, cluwords_data, algorithm_type)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "# Fit the NMF model\n",
    "print(\"\\nFitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_documents, n_cluwords))\n",
    "nmf = NMF(n_components=n_components,\n",
    "          random_state=1,\n",
    "          alpha=.1,\n",
    "          l1_ratio=.5).fit(cluwords_tfidf)\n",
    "\n",
    "end = timeit.default_timer()\n",
    "print(\"NMF done in {}.\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/matrix_w.txt'.format(path_to_save_results), 'w') as f:\n",
    "    w = nmf.fit_transform(cluwords_tfidf)  # matrix W = m x k\n",
    "    h = nmf.components_.transpose()  # matrix H = n x k\n",
    "    print('W: {} H:{}'.format(w.shape, h.shape))\n",
    "    for x in range(w.shape[0]):\n",
    "        for y in range(w.shape[1]):\n",
    "            f.write('{} '.format(w[x][y]))\n",
    "        f.write('\\n')\n",
    "    f.close()\n",
    "    del w\n",
    "    del h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load topics\n",
    "topics = top_words(nmf, list(vocab_cluwords), 500)\n",
    "\n",
    "# Load Cluwords representation for metrics\n",
    "cluwords_freq, cluwords_docs, n_docs =count_tf_idf_repr(topics, vocab_cluwords, cluwords_tfidf.transpose())\n",
    "topics = parse_topics(topics)\n",
    "one_hot_topics = get_one_hot_topics(topics, 500, np.array(vocab_cluwords), dataset)\n",
    "_nearest_neighbors(one_hot_topics, documents, vocab_cluwords, n_components, dataset)\n",
    "topics = remove_redundant_words(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(cluwords_freq=cluwords_freq,\n",
    "              cluwords_docs=cluwords_docs,\n",
    "              path_to_save_results=path_to_save_results,\n",
    "              topics=topics,\n",
    "              n_docs=n_docs\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
