{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.spatial.distance as sci_dist\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 9.78 µs\n",
      "(8652, 4) docs shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'exploit photo locat direct cluster base point interest discoveri'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_input(dataset_file_path):\n",
    "    #arq = open(dataset_file_path, 'r')\n",
    "    #doc = arq.readlines()\n",
    "    #arq.close()\n",
    "\n",
    "    #documents = list(map(str.rstrip, doc))\n",
    "    #n_documents = len(documents)\n",
    "    \n",
    "    %time \n",
    "    df = pd.read_pickle(dataset_file_path, compression='xz')\n",
    "    print('{} docs shape'.format(df.shape))\n",
    "    documents = df['clean'].values.tolist()\n",
    "    string = ''\n",
    "    for index, document in enumerate(documents):\n",
    "        for word in document:\n",
    "            string += word + ' '\n",
    "        string = string[:-1]\n",
    "        documents[index] = string \n",
    "        string = ''\n",
    "    n_documents = len(documents)\n",
    "    return documents, n_documents\n",
    "documents, n_documents = read_input('2017_lattes.pkl.xz')\n",
    "#documents, n_documents = read_input('2017_lattesPre.txt')\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(docPre):\n",
    "        # print docPre\n",
    "        count_vec = CountVectorizer(binary=True)  # , min_df=0.0,max_df=1.0, token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "        count = count_vec.fit_transform(docPre)\n",
    "        words = list(map(str, count_vec.get_feature_names()))\n",
    "\n",
    "        # Sem print words nao funciona kkk\n",
    "        # print words\n",
    "        # input(\"ok\")\n",
    "        # exit()\n",
    "\n",
    "        n_terms = len(words)\n",
    "\n",
    "        count_t = count.transpose()\n",
    "\n",
    "        word_frequency = {}\n",
    "        term_docs = {}\n",
    "\n",
    "        # print type(count)\n",
    "        # trocar essa parte (change)\n",
    "        for i in range(n_terms):\n",
    "            word_frequency[words[i]] = float(count_t[i].getnnz(1))\n",
    "            term_docs[words[i]] = set(count_t[i].nonzero()[1])\n",
    "\n",
    "        # print term_docs\n",
    "\n",
    "        # word_frequency['mm'] = word_frequency['tum']\n",
    "        # term_docs['mm'] = term_docs['tum']\n",
    "\n",
    "        # print word_frequency[\"video\"]\n",
    "        # print word_frequency[\"call\"]\n",
    "\n",
    "        return n_terms, words, word_frequency, term_docs\n",
    "n_terms, words, word_frequency, term_docs = count(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(topics_path):\n",
    "    arq = open(topics_path, 'r')\n",
    "    topics = arq.readlines()\n",
    "    arq.close()\n",
    "    return topics\n",
    "topics = get_topics('LDA_Topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coherence(topic, word_frequency, term_docs):\n",
    "    coherence = []\n",
    "\n",
    "    for t in range(len(topic)):\n",
    "        topico = topic[t]\n",
    "        top_w = topico.split(\" \")\n",
    "\n",
    "        coherence_t = 0.0\n",
    "        for i in range(1, len(top_w)):\n",
    "            for j in range(0, i):\n",
    "                cont_wi = word_frequency[top_w[j]]\n",
    "                cont_wi_wj = float(\n",
    "                    len(term_docs[top_w[j]].intersection(term_docs[top_w[i]])))\n",
    "                coherence_t += np.log((cont_wi_wj + 1.0) / cont_wi)\n",
    "\n",
    "        coherence.append(coherence_t)\n",
    "\n",
    "    return coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi(topics, word_frequency, term_docs, n_docs, n_top_words):\n",
    "    pmi = []\n",
    "    npmi = []\n",
    "\n",
    "    n_top_words = float(n_top_words)\n",
    "\n",
    "    for t in range(len(topics)):\n",
    "        top_w = topics[t]\n",
    "        # top_w = topico.split(' ')\n",
    "\n",
    "        pmi_t = 0.0\n",
    "        npmi_t = 0.0\n",
    "\n",
    "        for j in range(1, len(top_w)):\n",
    "            for i in range(0, j):\n",
    "                ti = top_w[i]\n",
    "                tj = top_w[j]\n",
    "\n",
    "                c_i = word_frequency[ti]\n",
    "                c_j = word_frequency[tj]\n",
    "                c_i_and_j = len(term_docs[ti].intersection(term_docs[tj]))\n",
    "\n",
    "                pmi_t += np.log(((c_i_and_j + 1.0) / float(n_docs)) /\n",
    "                                ((c_i * c_j) / float(n_docs) ** 2))\n",
    "\n",
    "                npmi_t += -1.0 * np.log((c_i_and_j + 0.01) / float(n_docs))\n",
    "\n",
    "        peso = 1.0 / (n_top_words * (n_top_words - 1.0))\n",
    "\n",
    "        pmi.append(peso * pmi_t)\n",
    "        npmi.append(pmi_t / npmi_t)\n",
    "\n",
    "        return pmi, npmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_metric(topics, t, path_to_save_model, distance_type, dataset, embedding_type=False):\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(\n",
    "        fname='{}/{}.txt'.format(path_to_save_model, dataset), binary=embedding_type)\n",
    "    model = word_vectors.wv\n",
    "    values = []\n",
    "\n",
    "    for topic in topics:\n",
    "        words = topic.split(' ')\n",
    "        value = Evaluation._calc_dist_2(words, model, distance_type, t)\n",
    "        values.append(value)\n",
    "\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "arq = open('dataset.txt', 'w')\n",
    "for doc in documents:\n",
    "    arq.write(doc)\n",
    "arq.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'virtua'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8fd8cecf2f46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mf_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mcoherence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoherence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_frequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mf_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Coherence: {} ({})\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoherence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoherence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mf_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoherence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-1f531f1b709d>\u001b[0m in \u001b[0;36mcoherence\u001b[0;34m(topic, word_frequency, term_docs)\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mcont_wi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_frequency\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 cont_wi_wj = float(\n\u001b[0;32m---> 13\u001b[0;31m                     len(term_docs[top_w[j]].intersection(term_docs[top_w[i]])))\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0mcoherence_t\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcont_wi_wj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mcont_wi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'virtua'"
     ]
    }
   ],
   "source": [
    "t = 10\n",
    "with open('{}/result_topic_{}.txt'.format('.', t), 'w') as f_res:\n",
    "    f_res.write('Topics {}\\n'.format(t))\n",
    "    f_res.write('Topics:\\n')\n",
    "    topics_t = []\n",
    "    for topic in topics:\n",
    "        topics_t.append(topic[:t])\n",
    "        for word in topic[:t]:\n",
    "            f_res.write('{} '.format(word))\n",
    "\n",
    "        f_res.write('\\n')\n",
    "\n",
    "    coherence = coherence(topics_t, word_frequency, term_docs)\n",
    "    f_res.write('Coherence: {} ({})\\n'.format(np.round(np.mean(coherence), 4), np.round(np.std(coherence), 4)))\n",
    "    f_res.write('{}\\n'.format(coherence))\n",
    "\n",
    "    pmi, npmi = pmi(topics=topics_t,\n",
    "                               word_frequency=word_frequency,\n",
    "                               term_docs=term_docs,\n",
    "                               n_docs=n_docs,\n",
    "                               n_top_words=t)\n",
    "    # f_res.write('PMI: {} ({})\\n'.format(np.round(np.mean(pmi), 4), np.round(np.std(pmi), 4)))\n",
    "    # f_res.write('{}\\n'.format(pmi))\n",
    "    f_res.write('NPMI:\\n')\n",
    "    for score in npmi:\n",
    "        f_res.write('{}\\n'.format(score))\n",
    "\n",
    "    f_res.write('avg NPMI: {} ({})\\n'.format(np.round(np.mean(npmi), 4), np.round(np.std(npmi), 4)))\n",
    "\n",
    "    # w2v_l1 = Evaluation.w2v_metric(topics, t, path_to_save_model, 'l1_dist', dataset)\n",
    "    # f_res.write('W2V-L1: {} ({})\\n'.format(np.round(np.mean(w2v_l1), 4), np.round(np.std(w2v_l1), 4)))\n",
    "    # f_res.write('{}\\n'.format(w2v_l1))\n",
    "\n",
    "    f_res.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
